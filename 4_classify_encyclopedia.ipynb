{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "from util.local_data_handler import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLF_MODEL_FILENAME = \"logistic_regression.joblib\"\n",
    "clf = load(CLF_MODEL_FILENAME)\n",
    "editions_embedded = load_from_disk(\"data/json/classification/encyclopedia_embeds.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Classifier to Encyclopedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_first_ed = editions_embedded['first_ed']\n",
    "X_fourth_ed = editions_embedded['fourth_ed']\n",
    "\n",
    "X_first_ed, X_fourth_ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_first_ed = clf.predict(X_first_ed['CLS_embed'])\n",
    "pred_fourth_ed = clf.predict(X_fourth_ed['CLS_embed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for 'Person' Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 1=True and 0=False boolean lists for numpy filtering\n",
    "people_filter_first_ed = [bool(i) for i in pred_first_ed]\n",
    "people_filter_fourth_ed = [bool(i) for i in pred_fourth_ed]\n",
    "\n",
    "non_people_filter_first_ed = [not bool(i) for i in pred_first_ed]\n",
    "non_people_filter_fourth_ed = [not bool(i) for i in pred_fourth_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1_people = np.array(X_first_ed['text'])[people_filter_first_ed]\n",
    "text_1_non_people = np.array(X_first_ed['text'])[non_people_filter_first_ed]\n",
    "text_4_people = np.array(X_fourth_ed['text'])[people_filter_fourth_ed]\n",
    "text_4_non_people = np.array(X_fourth_ed['text'])[non_people_filter_fourth_ed]\n",
    "\n",
    "headword_1_people = np.array(X_first_ed['headword'])[people_filter_first_ed]\n",
    "headword_1_non_people = np.array(X_first_ed['headword'])[non_people_filter_first_ed]\n",
    "headword_4_people = np.array(X_fourth_ed['headword'])[people_filter_fourth_ed]\n",
    "headword_4_non_people = np.array(X_fourth_ed['headword'])[non_people_filter_fourth_ed]\n",
    "\n",
    "entryId_1_people = np.array(X_first_ed['entryId'])[people_filter_first_ed]\n",
    "entryId_1_non_people = np.array(X_first_ed['entryId'])[non_people_filter_first_ed]\n",
    "entryId_4_people = np.array(X_fourth_ed['entryId'])[people_filter_fourth_ed]\n",
    "entryId_4_non_people = np.array(X_fourth_ed['entryId'])[non_people_filter_fourth_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FIRST ED:  {len(text_1_people)} entries classified as 'Person' out of {len(X_first_ed)} entries ({round((len(text_1_people)/len(X_first_ed))*100, 2)}%)\")\n",
    "print(f\"FOURTH ED: {len(text_4_people)} entries classified as 'Person' out of {len(X_fourth_ed)} entries ({round((len(text_4_people)/len(X_fourth_ed))*100, 2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Summing the stacked parts\n",
    "first_ed_people_sum = len(text_1_people)\n",
    "first_ed_non_people_sum = len(X_first_ed) - first_ed_people_sum\n",
    "fourth_ed_people_sum = len(text_4_people)\n",
    "fourth_ed_non_people_sum = len(X_fourth_ed) - fourth_ed_people_sum\n",
    "\n",
    "# X-axis positions\n",
    "x = np.arange(2)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# First column with stacks\n",
    "# ax.bar(0, first_ed_people_sum, label='Entries', color='tomato')\n",
    "ax.bar(0, len(X_first_ed), label='Entries', color='gray')\n",
    "# ax.bar(0, first_ed_non_people_sum, bottom=first_ed_people_sum, label='Non-Person', color='royalblue')\n",
    "\n",
    "# Second column with stacks\n",
    "# ax.bar(1, fourth_ed_people_sum, label='Entries', color='tomato')\n",
    "ax.bar(1, len(X_fourth_ed), label='Entries', color='gray')\n",
    "# ax.bar(1, fourth_ed_non_people_sum, bottom=fourth_ed_people_sum, label='Non-Person', color='royalblue')\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['1st edition', '4th edition'])\n",
    "ax.set_ylabel('No. of Entries')\n",
    "# ax.set_title('Person/Non-Person')\n",
    "ax.set_title('Total no. of Entries Per Edition')\n",
    "ax.legend()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "unique_labels = dict(zip(labels, handles))\n",
    "ax.legend(unique_labels.values(), unique_labels.keys())\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Collect Person Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_RESULTS = False\n",
    "\n",
    "FIRST_ED_PEOPLE_LOCATION = \"data/json/classification/first_ed_people.json\"\n",
    "FOURTH_ED_PEOPLE_LOCATION = \"data/json/classification/fourth_ed_people.json\"\n",
    "\n",
    "FIRST_ED_NON_PEOPLE_LOCATION = \"data/json/classification/first_ed_non_people.json\"\n",
    "FOURTH_ED_NON_PEOPLE_LOCATION = \"data/json/classification/fourth_ed_non_people.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all entries labelled as 'person'\n",
    "person_entries_first_ed = []\n",
    "for i in range(len(text_1_people)):\n",
    "    new_entry = {\n",
    "        \"headword\": headword_1_people[i],\n",
    "        \"entryId\":entryId_1_people[i],\n",
    "        \"text\": text_1_people[i],\n",
    "        \"person\": 1,\n",
    "        \"qid\": \"\"\n",
    "    }\n",
    "    person_entries_first_ed.append(new_entry)\n",
    "\n",
    "non_person_entries_first_ed = []\n",
    "for i in range(len(text_1_non_people)):\n",
    "    new_entry = {\n",
    "        \"headword\": headword_1_non_people[i],\n",
    "        \"entryId\":entryId_1_non_people[i],\n",
    "        \"text\": text_1_non_people[i],\n",
    "        \"person\": 0,\n",
    "        \"qid\": \"\"\n",
    "    }\n",
    "    non_person_entries_first_ed.append(new_entry)\n",
    "\n",
    "random.sample(person_entries_first_ed, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_entries_fourth_ed = []\n",
    "for i in range(len(text_4_people)):\n",
    "    new_entry = {\n",
    "        \"headword\": headword_4_people[i],\n",
    "        \"entryId\": entryId_4_people[i],\n",
    "        \"text\": text_4_people[i],\n",
    "        \"person\": 1,\n",
    "        \"qid\": \"\"\n",
    "    }\n",
    "    person_entries_fourth_ed.append(new_entry)\n",
    "\n",
    "non_person_entries_fourth_ed = []\n",
    "for i in range(len(text_4_non_people)):\n",
    "    new_entry = {\n",
    "        \"headword\": headword_4_non_people[i],\n",
    "        \"entryId\": entryId_4_non_people[i],\n",
    "        \"text\": text_4_non_people[i],\n",
    "        \"person\": 0,\n",
    "        \"qid\": \"\"\n",
    "    }\n",
    "    non_person_entries_fourth_ed.append(new_entry)\n",
    "\n",
    "random.sample(person_entries_fourth_ed, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_RESULTS:\n",
    "    with open(FIRST_ED_PEOPLE_LOCATION, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(person_entries_first_ed, outfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "    with open(FIRST_ED_NON_PEOPLE_LOCATION, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(non_person_entries_first_ed, outfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "    with open(FOURTH_ED_PEOPLE_LOCATION, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(person_entries_fourth_ed, outfile, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    with open(FOURTH_ED_NON_PEOPLE_LOCATION, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(non_person_entries_fourth_ed, outfile, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
