{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch\n",
    "import numpy as np\n",
    "from util.local_data_handler import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load KB-Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODEL_CKPT = \"./kb-bert-base-swedish-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_CKPT, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(LOCAL_MODEL_CKPT, local_files_only=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD_LOCAL = None\n",
    "LOAD_LOCAL = \"data/json/training/ver1/dataset_1_embeds.hf\"\n",
    "\n",
    "SAVE_LOCATION = None\n",
    "# SAVE_LOCATION = \"data/json/training/ver1/datasets_1_embeds.hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_CLS_embeds(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    # Return vector for [CLS] token\n",
    "    return {\"CLS_embed\": last_hidden_state[:,0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_JSON = \"data/json/training/ver1/dataset_1_training.json\"\n",
    "VALIDATION_DATA_JSON = \"data/json/training/ver1/dataset_1_validation.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_LOCAL:\n",
    "    entries_embedded = load_from_disk(LOAD_LOCAL)\n",
    "else:\n",
    "    data_files = {\n",
    "        \"train\": TRAINING_DATA_JSON,\n",
    "        \"validation\": VALIDATION_DATA_JSON\n",
    "    }\n",
    "\n",
    "    entries = load_dataset(\"json\", data_files=data_files)\n",
    "    entries_encoded = entries.map(tokenize, batched=True, batch_size=None)\n",
    "\n",
    "    entries_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    entries_embedded = entries_encoded.map(extract_CLS_embeds, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_LOCATION:\n",
    "    entries_embedded.save_to_disk(SAVE_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_embedded.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(entries_embedded[\"train\"][\"CLS_embed\"])\n",
    "y_train = np.array(entries_embedded[\"train\"][\"label\"])\n",
    "\n",
    "X_val = np.array(entries_embedded[\"validation\"][\"CLS_embed\"])\n",
    "y_val = np.array(entries_embedded[\"validation\"][\"label\"])\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# lr_clf = LogisticRegression()\n",
    "clf = LogisticRegressionCV(cv=cv, max_iter=3000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_val)\n",
    "X_val_text = entries_embedded[\"validation\"][\"text\"]\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] != y_val[i]:\n",
    "        print(f\"(label={y_val[i]}, pred={y_pred[i]}) {X_val_text[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, y_pred, target_names=[\"Non-Person\", \"Person\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "y_preds = clf.predict(X_val)\n",
    "plot_confusion_matrix(y_preds, y_val, [\"Non-Person\", \"Person\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_STRING = \"<b>Liedbeck</b> [lid-], Per Jakob, läkare och homöopat, född i Trosa d. 16 Juni 1802, död i Stockholm d. 5 Okt. 1876, blef student i Upsala 1821, med. licentiat 1828 och med\"\n",
    "# TEST_LABEL = 1\n",
    "\n",
    "test_string_encoded = tokenizer(TEST_STRING, return_tensors=\"pt\")\n",
    "\n",
    "inputs = {k:v.to(device) for k,v in test_string_encoded.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "test_string_embedded = outputs.last_hidden_state[:,0]\n",
    "\n",
    "clf.predict(test_string_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "CLF_MODEL_FILENAME = \"logistic_regression.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(clf, CLF_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenze+Embed the Whole Encyclopedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_ED = \"data/json/first_ed/first_ed.json\"\n",
    "FOURTH_ED = \"data/json/fourth_ed/fourth_ed.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Entire Encyclopedias\n",
    "# editions_data = load_dataset(\"json\", data_files={\"first_ed\": FIRST_ED, \"fourth_ed\": FOURTH_ED})\n",
    "\n",
    "# # Tokenize\n",
    "# editions_encoded = editions_data.map(tokenize, batched=True, batch_size=None)\n",
    "# # Model expects tensors as inputs: convert input_ids and attention_mask to \"torch\" format\n",
    "# editions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "# # Extract first columns of last hidden states (CLS vectors)\n",
    "# editions_embedded = editions_encoded.map(extract_CLS_embeds, batched=True)\n",
    "\n",
    "# editions_embedded.save_to_disk(\"data/json/classification/encyclopedia_embeds.hf\")\n",
    "editions_embedded = load_from_disk(\"data/json/classification/encyclopedia_embeds.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editions_embedded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
